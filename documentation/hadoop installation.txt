Download VMware 12.1.1
=>Install Oracle VM Virtual Box. Keep Default settings.



Ubuntu Installation Steps:	
1.	Download UBUNTU 16.04.1 from 
	https://www.ubuntu.com/download/desktop/thank-you?country=IN&version=16.04.2&architecture=amd64 
2.	Install Ubuntu64ServerBasic (OVA-oracle virtual application).
3.	open VMware-->click on create new vertual m/c-->select Installer disk image file and browse ubuntu ISO file-->next
4.	give user id and password-->next
5.      give vertual m/c name-->browse the locaton-->next
6.	select the disk size and split vertual m/c in multiple files-->next-->next--> finish
7.	u can change setting of vertual m/c from edit vertual m/c setting
8.double click to start vertual m/c

9.Type username:hduser
Pwd:hduser
Now,we can run the linus commands on it.

Run the command:ifconfig eth0->to check the IP of virtual machine.
192.168.56.123

Installing Java
check current location
=>pwd

# Update the source list
=> sudo apt-get update

# The OpenJDK project is the default version of Java 
# that is provided from a supported Ubuntu repository.	
=> sudo apt-get install default-jdk

=> java -version
java version "1.8"
OpenJDK Runtime Environment (IcedTea 2.5.3) (7u71-2.5.3-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)


INSTALL HADOOP
download hadoop 2.7.1 tar file in UBUNTU
Now,install Hadoop into linux terminal,
We are using 2.7.1 version of Hadoop,


We can check the location,
=>pwd
/home/hduser
Extract this tar file on linux by running the following command:
tar -xzvf hadoop-2.7.1.tar.gz
x-extract
z-dual archieve,more than one time file has been archieved.
v-virbose,Whatever it extracts,it will show every step.
f-forcefully,till the time it don’t extract final folder,keeps extracting.
Now, check it that hadoop folder has been extracted

Now,specify in Linux, where the Hadoop is installed.For this,we need to set the Path.For this,we have environment file in Linux.
To set the environment variable,use bashrc file.
=>sudo nano ~/.bashrc
sudo->admin  rights.
nano->edit the file.
==>home directory
We have bash-profile file also which contains the data of per profile.
Bashrc is at the upper level and bash –profile comes under bashrc which contains the login information of individual users.

---->sudo nano ~/.bashrc
When this file gets opened,set the following environment variables;


sudo nano ~/.bashrc
Note:Use Ctrl+x to come out of the editor.

Append the following variables in this file:

export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
export HADOOP_PREFIX="/home/hduser/hadoop-2.7.1/"
export PATH=$PATH:$HADOOP_PREFIX/bin
export PATH=$PATH:$HADOOP_PREFIX/sbin
export HADOOP_COMMON_HOME=${HADOOP_PREFIX}
export HADOOP_MAPRED_HOME=${HADOOP_PREFIX}y
export HADOOP_HDFS_HOME=${HADOOP_PREFIX}
export YARN_HOME=${HADOOP_PREFIX}

After saving the file, run the source command to refresh the values of the environment variables.

--->source ~/.bashrc

1)Edit the /home/hduser/hadoop-2.7.1/etc/hadoop/core-site.xml file and add the Hadoop HDFS URI (NameNode and its port) 
 property under configuration tag.

To edit type,nano filepath(e.g. nano /home/hduser/hadoop-2.7.1/etc/hadoop/core-site.xml)
Note:while doing the copy and paste of the command,no space  should be there,otherwise the node may not start.
 
   <property>
        <name>fs.defaultFS</name>
        <value>hdfs://192.168.88.128:8020</value>
        <final>true</final>
    </property>

2)Edit the /home/hduser/hadoop-2.7.1/etc/hadoop/hdfs-site.xml file and add the Hadoop HDFS properties:

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///home/hduser/hadoop-2.7.1/hadoop_data/dfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/hduser/hadoop-2.7.1/hadoop_data/dfs/datanode</value>
    </property>
</configuration>

3)Rename the mapred-site.xml.template file through  WinScp to mapred-site.xml
Edit the /home/hduser/hadoop-2.7.1/etc/hadoop/mapred-site.xml file and specify the MapReduce framework as YARN.

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

4)Edit the /home/hduser/hadoop-2.7.1/etc/hadoop/yarn-site.xml file and specify the YARN properties.

<configuration>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>192.168.88.128:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>192.168.88.128:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>192.168.88.128:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>192.168.88.128:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>192.168.88.128:8088</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
	<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    </property>
</configuration>

5)Edit the /home/hduser/hadoop-2.7.1/etc/hadoop/hadoop env.sh file and specify the env fike properties.

Append the following variables in this file:

export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64

Last step before running the Hadoop services is to format the NameNode. Before executing the format command, 
make sure that the dfs.namenode.name.dir and dfs.datanode.data.dir directories specified in hdfs-site.xml file  does not exists. The hdfs command exists in the /home/hduser/hadoop-2.7.1/bin directory.

hdfs namenode –format
It will generate the folder named hadoop_data we can check it in winscp .The path where it stores is:
/home/hduser/hadoop-2.7.1…Refresh once only then it will be visible.
Now,Hadoop has been installed .Run below commands to start the services on Hadoop.

->JPS is the command to check which all services are running.
->jps
Service	Command
Namenode	hadoop-daemon.sh start namenode
Datanode	hadoop-daemon.sh start datanode
Resourcemanager	yarn-daemon.sh start resourcemanager
Nodemanager	yarn-daemon.sh start nodemanager

Check with jps whether all the services has started or not.
After that,open chrome
http://192.168.88.128:50070/
HDFS  will be visible .Go to Browse->utilities to see all files.







